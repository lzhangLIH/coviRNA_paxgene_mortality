{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f965a758-5e3f-4095-8639-d778d0d04837",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import copy\n",
    "\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV, GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn import metrics\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning, append=True)\n",
    "\n",
    "# from scikit-beam.utils.exceptions import Scikit-beamWarningg\n",
    "# warnings.simplefilter('ignore', category=Scikit-beamWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fba8711-af94-44d9-9e4c-f57add296b6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## with balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d53cac1-ac21-459c-8264-dca8d5facd73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cv_tune_bl_split(dat_use, # dataframe with at leat one column name: outcome, one column name: ID\n",
    "                     outcome, # outcome column\n",
    "                     ID,# sample ID column\n",
    "                     var_, # selected variables\n",
    "                     key,# string in the list rf, knn, logit_l1, logit_l2, elasticnet, mlp, xgb, mlp and svm\n",
    "                     clf,\n",
    "                     grid,\n",
    "                     scoring='roc_auc', \n",
    "                     output_prefix = 'cv_tune', \n",
    "                     n_jobs=1, \n",
    "                     random_state=123, \n",
    "                     folder_out='/results/', \n",
    "                     kf_inner=5,\n",
    "                     perc_train=0.8):\n",
    "    \n",
    "    samples_1 = dat_use.loc[dat_use[outcome] == 1, ID].values\n",
    "    print(f'sample size group 1: {samples_1.shape}')\n",
    "    \n",
    "\n",
    "    samples_0 = dat_use.loc[dat_use[outcome] == 0, ID].values\n",
    "    print(f'sample size group 0: {samples_0.shape}')\n",
    "    \n",
    "    n_train = math.ceil(min(len(samples_0), len(samples_1))*perc_train)\n",
    "\n",
    "    print(key)\n",
    "    eval_metrics = pd.DataFrame()\n",
    "\n",
    "    for i in range(100):\n",
    "        # samples of group 1 in test dataset\n",
    "        samples_1_train = resample(samples_1, replace=(False), n_samples=n_train, random_state=(i))\n",
    "        # samples_1_test = resample(samples_1, replace=(False), n_samples=n_test, random_state=(i))\n",
    "        samples_1_test = samples_1[np.isin(samples_1, samples_1_train, invert=True)]\n",
    "\n",
    "        # samples of group 0, the same number of training and test as in for group 1 from group 0\n",
    "        samples_0_train = resample(samples_0, replace=(False), n_samples=n_train, random_state=(i))\n",
    "        # samples_0_test = resample(samples_0, replace=(False), n_samples=n_test, random_state=(i))\n",
    "        samples_0_ = samples_0[np.isin(samples_0, samples_0_train, invert=True)]\n",
    "        samples_0_test = resample(samples_0_, replace=(False), n_samples=len(samples_1_test), random_state=(i))\n",
    "\n",
    "        ##### \n",
    "        X_train = dat_use.loc[np.append(samples_0_train, samples_1_train), var_].values\n",
    "        print(X_train.shape)\n",
    "        y_train = dat_use.loc[np.append(samples_0_train, samples_1_train), outcome].values\n",
    "        print(Counter(y_train))\n",
    "\n",
    "        X_test = dat_use.loc[np.append(samples_0_test, samples_1_test), var_].values\n",
    "        print(X_test.shape)\n",
    "        y_test = dat_use.loc[np.append(samples_0_test, samples_1_test), outcome].values\n",
    "        print(Counter(y_test))\n",
    "\n",
    "        pipeline = Pipeline([('scale', PowerTransformer()), #####\n",
    "                                # (\"smote\", SMOTE(random_state=random_state, n_jobs=n_jobs)), \n",
    "                              (\"clf\", clf)])\n",
    "\n",
    "        # search = RandomizedSearchCV(\n",
    "        #     pipeline, grid, \n",
    "        #     # scoring=metrics.make_scorer(metrics.matthews_corrcoef),\n",
    "        #     scoring=scoring,\n",
    "        #     n_iter=100,random_state=random_state,\n",
    "        #     n_jobs=n_jobs, cv=kf_inner,return_train_score=True\n",
    "        # ).fit(X_train, y_train)\n",
    "\n",
    "        search = GridSearchCV(\n",
    "            pipeline, grid, \n",
    "            # scoring=metrics.make_scorer(metrics.matthews_corrcoef),\n",
    "            scoring=scoring, \n",
    "            n_jobs=n_jobs, cv=kf_inner,return_train_score=True\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "        best_score = search.best_score_\n",
    "        # print(f\"Best Tuning balanced accuracy: {best_score}\") ###############\n",
    "\n",
    "        best_params = {\n",
    "            key: value for key, value in search.best_params_.items()\n",
    "        }\n",
    "        # print(best_params)\n",
    "\n",
    "        best_estimator = search.best_estimator_\n",
    "\n",
    "        df_proba = pd.DataFrame()\n",
    "\n",
    "        ave = 'macro'\n",
    "\n",
    "        # y_pred_cv = np.append(y_pred_cv, y_pred)\n",
    "\n",
    "        # area under ROC curve\n",
    "        y_probs = best_estimator.predict_proba(X_test)[:,1]\n",
    "        # y_prob_cv = np.append(y_prob_cv, y_probs)\n",
    "\n",
    "        fp_rate, tp_rate, thresholds = metrics.roc_curve(y_test, y_probs)\n",
    "        auc = metrics.auc(fp_rate, tp_rate)\n",
    "        # auc_weight = metrics.roc_auc_score(y_test, y_pred_weight)\n",
    "\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "        # # probability\n",
    "        # df_proba_ = pd.DataFrame()\n",
    "        # df_proba_['proba'] = y_probs\n",
    "        # df_proba_['true'] = y_test\n",
    "        # df_proba = df_proba.append(df_proba_, ignore_index=True)\n",
    "\n",
    "        # area under precision-recall curve\n",
    "        precision_, recall_, thresholds_ = metrics.precision_recall_curve(y_test, y_probs)\n",
    "        auc_pr = metrics.auc(recall_, precision_)\n",
    "\n",
    "        precision = metrics.precision_score(y_test, y_pred, average=ave)\n",
    "        precision_1 = metrics.precision_score(y_test, y_pred)\n",
    "        precision_0 = metrics.precision_score(y_test, y_pred, pos_label=0)\n",
    "        f1 = metrics.f1_score(y_test, y_pred, average=ave)\n",
    "        f1_1 = metrics.f1_score(y_test, y_pred)\n",
    "        f1_0 = metrics.f1_score(y_test, y_pred, pos_label=0)\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        mcc = metrics.matthews_corrcoef(y_test, y_pred)\n",
    "        recall = metrics.recall_score(y_test, y_pred, average=ave)\n",
    "        recall_1 = metrics.recall_score(y_test, y_pred)\n",
    "        recall_0 = metrics.recall_score(y_test, y_pred, pos_label=0)\n",
    "        balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "        brier = metrics.brier_score_loss(y_test, y_probs)\n",
    "\n",
    "        eval_metrics = pd.concat([eval_metrics,pd.DataFrame.from_dict({'auroc': auc,\n",
    "                                            'accuracy': accuracy,\n",
    "                                            'sensitivity': recall_1,\n",
    "                                            'specificity': recall_0,\n",
    "                                            'brier': brier}, orient='index').transpose()], ignore_index=True)    \n",
    "    file_out = output_prefix+key+'.csv' ###############\n",
    "    eval_metrics.to_csv(os.path.join(folder_out,file_out), index=False)\n",
    "    \n",
    "    return eval_metrics\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b1b8474-28dc-4ec1-9b4d-9e9cfa0ea4d9",
   "metadata": {},
   "source": [
    "## with balanced dataset and split index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60f3eb7c-4fbe-491f-8343-0e94976c2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_tune_bl_idx(dat_use,# dataframe with at leat one column name: outcome, one column name: ID\n",
    "                   outcome, # outcome column\n",
    "                   ID, # sample ID column\n",
    "                   var_, # selected variables\n",
    "                   key, # string in the list rf, knn, logit_l1, logit_l2, elasticnet, mlp, xgb, mlp and svm\n",
    "                   train_test, # balanced train-test split index, 1=train, 0=test\n",
    "                   scoring='roc_auc', \n",
    "                   output_prefix = 'cv_tune', \n",
    "                   n_jobs=1, \n",
    "                   random_state=123, \n",
    "                   folder_out='/results/', \n",
    "                   kf_inner=5):\n",
    "    \n",
    "    clf = lst_clf[key]\n",
    "    \n",
    "    samples_1 = dat_use.loc[dat_use[outcome] == 1, ID].values\n",
    "    print(f'sample size group 1: {samples_1.shape}')\n",
    "\n",
    "    samples_0 = dat_use.loc[dat_use[outcome] == 0, ID].values\n",
    "    print(f'sample size group 0: {samples_0.shape}')\n",
    "\n",
    "    print(key)\n",
    "    eval_metrics = pd.DataFrame()\n",
    "\n",
    "    for i in range(100):\n",
    "        # train test index from outside\n",
    "        samples_train = train_test.loc[train_test['train_'+str(i+1)] == 1,ID].values\n",
    "        samples_test = train_test.loc[train_test['train_'+str(i+1)] == 0,ID].values ##### balanced test dataset or not\n",
    "        # samples_train.shape\n",
    "        # samples_test.shape\n",
    "\n",
    "        samples_1_train = samples_train[np.isin(samples_train, samples_1)]\n",
    "        samples_1_test = samples_test[np.isin(samples_test, samples_1)]\n",
    "\n",
    "        samples_0_train = samples_train[np.isin(samples_train, samples_0)]\n",
    "        samples_0_test = samples_test[np.isin(samples_test, samples_0)]\n",
    "\n",
    "        ##### \n",
    "        X_train = dat_use.loc[np.append(samples_0_train, samples_1_train), var_].values\n",
    "        print(X_train.shape)\n",
    "        y_train = dat_use.loc[np.append(samples_0_train, samples_1_train), outcome].values\n",
    "        print(Counter(y_train))\n",
    "\n",
    "        X_test = dat_use.loc[np.append(samples_0_test, samples_1_test), var_].values\n",
    "        print(X_test.shape)\n",
    "        y_test = dat_use.loc[np.append(samples_0_test, samples_1_test), outcome].values\n",
    "        print(Counter(y_test))\n",
    "\n",
    "        pipeline = Pipeline([('scale', PowerTransformer()), #####\n",
    "                                # (\"smote\", SMOTE(random_state=random_state, n_jobs=n_jobs)), \n",
    "                              (\"clf\", clf)])\n",
    "\n",
    "        grid = lst_grid[key]\n",
    "\n",
    "        # search = RandomizedSearchCV(\n",
    "        #     pipeline, grid, \n",
    "        #     # scoring=metrics.make_scorer(metrics.matthews_corrcoef),\n",
    "        #     scoring=scoring,\n",
    "        #     n_iter=100,random_state=random_state,\n",
    "        #     n_jobs=n_jobs, cv=kf_inner,return_train_score=True\n",
    "        # ).fit(X_train, y_train)\n",
    "\n",
    "        search = GridSearchCV(\n",
    "            pipeline, grid, \n",
    "            # scoring=metrics.make_scorer(metrics.matthews_corrcoef),\n",
    "            scoring=scoring, \n",
    "            n_jobs=n_jobs, cv=kf_inner,return_train_score=True\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "        best_score = search.best_score_\n",
    "        # print(f\"Best Tuning balanced accuracy: {best_score}\") ###############\n",
    "\n",
    "        best_params = {\n",
    "            key: value for key, value in search.best_params_.items()\n",
    "        }\n",
    "        # print(best_params)\n",
    "\n",
    "        best_estimator = search.best_estimator_\n",
    "\n",
    "        df_proba = pd.DataFrame()\n",
    "\n",
    "        ave = 'macro'\n",
    "\n",
    "        # y_pred_cv = np.append(y_pred_cv, y_pred)\n",
    "\n",
    "        # area under ROC curve\n",
    "        y_probs = best_estimator.predict_proba(X_test)[:,1]\n",
    "        # y_prob_cv = np.append(y_prob_cv, y_probs)\n",
    "\n",
    "        fp_rate, tp_rate, thresholds = metrics.roc_curve(y_test, y_probs)\n",
    "        auc = metrics.auc(fp_rate, tp_rate)\n",
    "        # auc_weight = metrics.roc_auc_score(y_test, y_pred_weight)\n",
    "\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "        # # probability\n",
    "        # df_proba_ = pd.DataFrame()\n",
    "        # df_proba_['proba'] = y_probs\n",
    "        # df_proba_['true'] = y_test\n",
    "        # df_proba = df_proba.append(df_proba_, ignore_index=True)\n",
    "\n",
    "        # area under precision-recall curve\n",
    "        precision_, recall_, thresholds_ = metrics.precision_recall_curve(y_test, y_probs)\n",
    "        auc_pr = metrics.auc(recall_, precision_)\n",
    "\n",
    "        precision = metrics.precision_score(y_test, y_pred, average=ave)\n",
    "        precision_1 = metrics.precision_score(y_test, y_pred)\n",
    "        precision_0 = metrics.precision_score(y_test, y_pred, pos_label=0)\n",
    "        f1 = metrics.f1_score(y_test, y_pred, average=ave)\n",
    "        f1_1 = metrics.f1_score(y_test, y_pred)\n",
    "        f1_0 = metrics.f1_score(y_test, y_pred, pos_label=0)\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        mcc = metrics.matthews_corrcoef(y_test, y_pred)\n",
    "        recall = metrics.recall_score(y_test, y_pred, average=ave)\n",
    "        recall_1 = metrics.recall_score(y_test, y_pred)\n",
    "        recall_0 = metrics.recall_score(y_test, y_pred, pos_label=0)\n",
    "        balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "        brier = metrics.brier_score_loss(y_test, y_probs)\n",
    "\n",
    "        eval_metrics = pd.concat([eval_metrics,pd.DataFrame.from_dict({'auroc': auc,\n",
    "                                            'accuracy': accuracy,\n",
    "                                            'sensitivity': recall_1,\n",
    "                                            'specificity': recall_0,\n",
    "                                            'brier': brier}, orient='index').transpose()], ignore_index=True)    \n",
    "    file_out = output_prefix+key+'.csv' ###############\n",
    "    eval_metrics.to_csv(os.path.join(folder_out,file_out), index=False)\n",
    "    \n",
    "    return eval_metrics\n",
    "\n",
    "    # y_test_pred = pd.DataFrame(np.concatenate((y_test_cv.reshape((-1,1)), y_pred_cv.reshape((-1,1))), axis=1))\n",
    "    # y_test_pred.columns = ['test','pred']\n",
    "    # print(y_test_pred.shape)\n",
    "    # pred_out = 'y_test_pred_cv5_'+key+'.csv' ###################\n",
    "    # y_test_pred.to_csv(os.path.join(folder_out, pred_out), index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31a1848-5181-4701-9174-43c3e2a6f2bc",
   "metadata": {},
   "source": [
    "## with balanced dataset and feature loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e720eef-32e1-4c01-8a1e-b7e62ba9efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_tune_bl_varLoop(dat_use, # dataframe with at leat one column name: outcome, one column name: ID\n",
    "                       outcome, # outcome column\n",
    "                       ID, # sample ID column\n",
    "                       key, # string in the list rf, knn, logit_l1, logit_l2, elasticnet, mlp, xgb, mlp and svm\n",
    "                       train_test, # balanced train-test split index, 1=train, 0=test\n",
    "                       df_features, \n",
    "                       scoring='roc_auc', \n",
    "                       output_prefix = 'cv_tune', \n",
    "                       n_jobs=1, \n",
    "                       random_state=123, \n",
    "                       folder_out='/results/', \n",
    "                       kf_inner=5):\n",
    "\n",
    "    print(key)\n",
    "    \n",
    "    clf = lst_clf[key]\n",
    "    \n",
    "    samples_1 = dat_use.loc[dat_use[outcome] == 1, ID].values\n",
    "    print(f'sample size group 1: {samples_1.shape}')\n",
    "\n",
    "    samples_0 = dat_use.loc[dat_use[outcome] == 0, ID].values\n",
    "    print(f'sample size group 0: {samples_0.shape}')\n",
    "    \n",
    "    eval_metrics = pd.DataFrame()\n",
    "\n",
    "    for i in range(100):\n",
    "        # features selected in each iteration\n",
    "        var_ = df_features.loc[df_features['select_'+str(i+1)] == 1, 'feature'].values\n",
    "        print(var_.shape)\n",
    "\n",
    "        # train test index from outside\n",
    "        samples_train = train_test.loc[train_test['train_'+str(i+1)] == 1,ID].values\n",
    "        samples_test = train_test.loc[train_test['train_'+str(i+1)] == 0,ID].values ##### balanced test dataset or not\n",
    "        # samples_train.shape\n",
    "        # samples_test.shape\n",
    "\n",
    "        samples_1_train = samples_train[np.isin(samples_train, samples_1)]\n",
    "        samples_1_test = samples_test[np.isin(samples_test, samples_1)]\n",
    "\n",
    "        samples_0_train = samples_train[np.isin(samples_train, samples_0)]\n",
    "        samples_0_test = samples_test[np.isin(samples_test, samples_0)]\n",
    "\n",
    "        ##### \n",
    "        X_train = dat_use.loc[np.append(samples_0_train, samples_1_train), var_].values\n",
    "        print(X_train.shape)\n",
    "        y_train = dat_use.loc[np.append(samples_0_train, samples_1_train), outcome].values\n",
    "        print(Counter(y_train))\n",
    "\n",
    "        X_test = dat_use.loc[np.append(samples_0_test, samples_1_test), var_].values\n",
    "        print(X_test.shape)\n",
    "        y_test = dat_use.loc[np.append(samples_0_test, samples_1_test), outcome].values\n",
    "        print(Counter(y_test))\n",
    "\n",
    "        pipeline = Pipeline([('scale', PowerTransformer()), #####\n",
    "                                # (\"smote\", SMOTE(random_state=random_state, n_jobs=n_jobs)), \n",
    "                              (\"clf\", clf)])\n",
    "\n",
    "        grid = lst_grid[key]\n",
    "\n",
    "        # search = RandomizedSearchCV(\n",
    "        #     pipeline, grid, \n",
    "        #     # scoring=metrics.make_scorer(metrics.matthews_corrcoef),\n",
    "        #     scoring=scoring,\n",
    "        #     n_iter=100,random_state=random_state,\n",
    "        #     n_jobs=n_jobs, cv=kf_inner,return_train_score=True\n",
    "        # ).fit(X_train, y_train)\n",
    "\n",
    "        search = GridSearchCV(\n",
    "            pipeline, grid, \n",
    "            # scoring=metrics.make_scorer(metrics.matthews_corrcoef),\n",
    "            scoring=scoring, \n",
    "            n_jobs=n_jobs, cv=kf_inner,return_train_score=True\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "        best_score = search.best_score_\n",
    "        # print(f\"Best Tuning balanced accuracy: {best_score}\") ###############\n",
    "\n",
    "        best_params = {\n",
    "            key: value for key, value in search.best_params_.items()\n",
    "        }\n",
    "        # print(best_params)\n",
    "\n",
    "        best_estimator = search.best_estimator_\n",
    "\n",
    "        df_proba = pd.DataFrame()\n",
    "\n",
    "        ave = 'macro'\n",
    "\n",
    "        # y_pred_cv = np.append(y_pred_cv, y_pred)\n",
    "\n",
    "        # area under ROC curve\n",
    "        y_probs = best_estimator.predict_proba(X_test)[:,1]\n",
    "        # y_prob_cv = np.append(y_prob_cv, y_probs)\n",
    "\n",
    "        fp_rate, tp_rate, thresholds = metrics.roc_curve(y_test, y_probs)\n",
    "        auc = metrics.auc(fp_rate, tp_rate)\n",
    "        # auc_weight = metrics.roc_auc_score(y_test, y_pred_weight)\n",
    "\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "        # # probability\n",
    "        # df_proba_ = pd.DataFrame()\n",
    "        # df_proba_['proba'] = y_probs\n",
    "        # df_proba_['true'] = y_test\n",
    "        # df_proba = df_proba.append(df_proba_, ignore_index=True)\n",
    "\n",
    "        # area under precision-recall curve\n",
    "        precision_, recall_, thresholds_ = metrics.precision_recall_curve(y_test, y_probs)\n",
    "        auc_pr = metrics.auc(recall_, precision_)\n",
    "\n",
    "        precision = metrics.precision_score(y_test, y_pred, average=ave)\n",
    "        precision_1 = metrics.precision_score(y_test, y_pred)\n",
    "        precision_0 = metrics.precision_score(y_test, y_pred, pos_label=0)\n",
    "        f1 = metrics.f1_score(y_test, y_pred, average=ave)\n",
    "        f1_1 = metrics.f1_score(y_test, y_pred)\n",
    "        f1_0 = metrics.f1_score(y_test, y_pred, pos_label=0)\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        mcc = metrics.matthews_corrcoef(y_test, y_pred)\n",
    "        recall = metrics.recall_score(y_test, y_pred, average=ave)\n",
    "        recall_1 = metrics.recall_score(y_test, y_pred)\n",
    "        recall_0 = metrics.recall_score(y_test, y_pred, pos_label=0)\n",
    "        balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "        brier = metrics.brier_score_loss(y_test, y_probs)\n",
    "\n",
    "        eval_metrics = pd.concat([eval_metrics,pd.DataFrame.from_dict({'auroc': auc,\n",
    "                                            'accuracy': accuracy,\n",
    "                                            'sensitivity': recall_1,\n",
    "                                            'specificity': recall_0,\n",
    "                                            'brier':brier}, orient='index').transpose()], ignore_index=True)    \n",
    "    file_out = output_prefix+key+'.csv' ###############\n",
    "    eval_metrics.to_csv(os.path.join(folder_out,file_out), index=False)\n",
    "    \n",
    "    return eval_metrics\n",
    "\n",
    "    # y_test_pred = pd.DataFrame(np.concatenate((y_test_cv.reshape((-1,1)), y_pred_cv.reshape((-1,1))), axis=1))\n",
    "    # y_test_pred.columns = ['test','pred']\n",
    "    # print(y_test_pred.shape)\n",
    "    # pred_out = 'y_test_pred_cv5_'+key+'.csv' ###################\n",
    "    # y_test_pred.to_csv(os.path.join(folder_out, pred_out), index=False)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd3f560-8239-4f17-a0f5-3ef025f3771a",
   "metadata": {},
   "source": [
    "## imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e706f-f4e6-4913-b361-e7358069cdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_tune_imbl(dat_use, \n",
    "                 var_, \n",
    "                 outcome,\n",
    "                 key, \n",
    "                 scoring='balanced_accuracy', \n",
    "                 output_prefix = 'cv_tune', \n",
    "                 n_jobs=1, \n",
    "                 random_state=123, \n",
    "                 folder_out='/results/', \n",
    "                 kf=5, \n",
    "                 kf_inner=5):\n",
    "\n",
    "    print(key)\n",
    "    \n",
    "    clf = lst_clf[key]\n",
    "    \n",
    "    data_X = dat_use[var_].values\n",
    "    data_y = dat_use[outcome].values\n",
    "    \n",
    "    eval_metrics = pd.DataFrame()\n",
    "\n",
    "    n = 0\n",
    "    for train_index, test_index in kf.split(data_X, data_y):\n",
    "        n = n+1\n",
    "        print('fold_'+str(n))\n",
    "\n",
    "        # training\n",
    "        X_train = data_X[train_index]\n",
    "        y_train = data_y[train_index]\n",
    "\n",
    "        # X_train = np.concatenate((X_train, X_train_))\n",
    "        # y_train = np.append(y_train, y_train_)\n",
    "\n",
    "        print(X_train.shape)\n",
    "        print(Counter(y_train))\n",
    "\n",
    "        # test\n",
    "        X_test = data_X[test_index]\n",
    "        y_test = data_y[test_index]\n",
    "        \n",
    "        print(X_test.shape)\n",
    "        print(Counter(y_test))\n",
    "\n",
    "\n",
    "        pipeline = Pipeline([('scale', PowerTransformer()), #####\n",
    "                                (\"smote\", RandomOverSampler(random_state=random_state)), \n",
    "                              (\"clf\", clf)])\n",
    "\n",
    "        grid = lst_grid[key]\n",
    "\n",
    "        # search = RandomizedSearchCV(\n",
    "        #     pipeline, grid, \n",
    "        #     # scoring=metrics.make_scorer(metrics.matthews_corrcoef),\n",
    "        #     scoring=scoring,\n",
    "        #     n_iter=100,random_state=random_state,\n",
    "        #     n_jobs=n_jobs, cv=kf_inner,return_train_score=True\n",
    "        # ).fit(X_train, y_train)\n",
    "\n",
    "        search = GridSearchCV(\n",
    "            pipeline, grid, \n",
    "            # scoring=metrics.make_scorer(metrics.matthews_corrcoef),\n",
    "            scoring=scoring, \n",
    "            n_jobs=n_jobs, cv=kf_inner,return_train_score=True\n",
    "        ).fit(X_train, y_train)\n",
    "\n",
    "        best_score = search.best_score_\n",
    "        # print(f\"Best Tuning balanced accuracy: {best_score}\") ###############\n",
    "\n",
    "        best_params = {\n",
    "            key: value for key, value in search.best_params_.items()\n",
    "        }\n",
    "        # print(best_params)\n",
    "\n",
    "        best_estimator = search.best_estimator_\n",
    "\n",
    "        df_proba = pd.DataFrame()\n",
    "\n",
    "        ave = 'macro'\n",
    "\n",
    "        # y_pred_cv = np.append(y_pred_cv, y_pred)\n",
    "\n",
    "        # area under ROC curve\n",
    "        y_probs = best_estimator.predict_proba(X_test)[:,1]\n",
    "        # y_prob_cv = np.append(y_prob_cv, y_probs)\n",
    "\n",
    "        fp_rate, tp_rate, thresholds = metrics.roc_curve(y_test, y_probs)\n",
    "        auc = metrics.auc(fp_rate, tp_rate)\n",
    "        # auc_weight = metrics.roc_auc_score(y_test, y_pred_weight)\n",
    "\n",
    "        y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "        # # probability\n",
    "        # df_proba_ = pd.DataFrame()\n",
    "        # df_proba_['proba'] = y_probs\n",
    "        # df_proba_['true'] = y_test\n",
    "        # df_proba = df_proba.append(df_proba_, ignore_index=True)\n",
    "\n",
    "        # area under precision-recall curve\n",
    "        precision_, recall_, thresholds_ = metrics.precision_recall_curve(y_test, y_probs)\n",
    "        auc_pr = metrics.auc(recall_, precision_)\n",
    "\n",
    "        precision = metrics.precision_score(y_test, y_pred, average=ave)\n",
    "        precision_1 = metrics.precision_score(y_test, y_pred)\n",
    "        precision_0 = metrics.precision_score(y_test, y_pred, pos_label=0)\n",
    "        f1 = metrics.f1_score(y_test, y_pred, average=ave)\n",
    "        f1_1 = metrics.f1_score(y_test, y_pred)\n",
    "        f1_0 = metrics.f1_score(y_test, y_pred, pos_label=0)\n",
    "        accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "        mcc = metrics.matthews_corrcoef(y_test, y_pred)\n",
    "        recall = metrics.recall_score(y_test, y_pred, average=ave)\n",
    "        recall_1 = metrics.recall_score(y_test, y_pred)\n",
    "        recall_0 = metrics.recall_score(y_test, y_pred, pos_label=0)\n",
    "        balanced_accuracy = metrics.balanced_accuracy_score(y_test, y_pred)\n",
    "\n",
    "        brier = metrics.brier_score_loss(y_test, y_probs)\n",
    "\n",
    "        eval_metrics = pd.concat([eval_metrics,pd.DataFrame.from_dict({'auroc': auc,\n",
    "                                            'sensitivity': recall_1,\n",
    "                                            'specificity': recall_0,\n",
    "                                            'balanced accuracy': balanced_accuracy,\n",
    "                                            'brier': brier}, orient='index').transpose()], ignore_index=True)    \n",
    "    file_out = output_prefix+key+'.csv' ###############\n",
    "    eval_metrics.to_csv(os.path.join(folder_out,file_out), index=False)\n",
    "    \n",
    "    return eval_metrics\n",
    "\n",
    "    # y_test_pred = pd.DataFrame(np.concatenate((y_test_cv.reshape((-1,1)), y_pred_cv.reshape((-1,1))), axis=1))\n",
    "    # y_test_pred.columns = ['test','pred']\n",
    "    # print(y_test_pred.shape)\n",
    "    # pred_out = 'y_test_pred_cv5_'+key+'.csv' ###################\n",
    "    # y_test_pred.to_csv(os.path.join(folder_out, pred_out), index=False)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "classification",
   "language": "python",
   "name": "classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
